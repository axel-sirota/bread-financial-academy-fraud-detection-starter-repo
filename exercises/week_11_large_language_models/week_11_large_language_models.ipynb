{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 11: Large Language Models (LLMs)\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this session, you will be able to:\n",
        "1. **Understand** what LLMs are and why they matter for data science\n",
        "2. **Call OpenAI and Anthropic APIs** for practical NLP tasks (classification, extraction)\n",
        "3. **Apply prompting techniques** (zero-shot, few-shot, chain-of-thought)\n",
        "4. **Use HuggingFace transformers** for local text generation and task prompting\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Watched pre-class videos on LLM architectures, transformers, tokenization\n",
        "- Completed the Week 6 fraud detection pipeline (Transcribe ‚Üí Comprehend ‚Üí XGBoost)\n",
        "- Familiarity with Python, pandas, basic ML concepts\n",
        "\n",
        "## What We'll Build Today\n",
        "\n",
        "Continuing our **fraud detection story** from weeks 5-7, we'll use LLMs to:\n",
        "- Classify transaction descriptions as potentially fraudulent\n",
        "- Extract structured fraud signals from customer dispute texts\n",
        "- Compare cloud API approaches (OpenAI, Anthropic) vs local models (HuggingFace)\n",
        "\n",
        "```\n",
        "Weeks 5-7 Approach          vs.        Week 11 Approach (Today)\n",
        "====================                   ========================\n",
        "\n",
        "  Call Transcript                        Call Transcript\n",
        "       |                                      |\n",
        "  [Transcribe] ‚îÄ‚îÄ‚ñ∫ Text                  [LLM API Call]\n",
        "       |                                      |\n",
        "  [Comprehend] ‚îÄ‚îÄ‚ñ∫ Sentiment,            Structured JSON:\n",
        "       |           Entities               - risk_level\n",
        "  [Feature Eng] ‚îÄ‚îÄ‚ñ∫ Numeric              - red_flags\n",
        "       |           Features               - confidence\n",
        "  [XGBoost] ‚îÄ‚îÄ‚ñ∫ fraud/legit              - reasoning\n",
        "                                          - action\n",
        "  Many steps, fixed outputs         One step, flexible outputs\n",
        "```\n",
        "\n",
        "## Session Format\n",
        "\n",
        "This session includes structured **peer discussion** moments marked with\n",
        "\"üí¨ Discussion\". When you see one, take 3-5 minutes to discuss with a\n",
        "neighbor before we reconvene as a group."
      ],
      "metadata": {
        "id": "cell-0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-1"
      },
      "source": [
        "# Section 0: Environment Setup\n",
        "\n",
        "We'll install and import libraries for three different LLM approaches:\n",
        "1. **OpenAI** SDK ‚Äî for calling GPT models via API\n",
        "2. **Anthropic** SDK ‚Äî for calling Claude models via API\n",
        "3. **HuggingFace Transformers** ‚Äî for running models locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-2",
        "outputId": "154ba45a-f921-4f1a-f60d-39a8bf8ac388",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# =============================================================================\n",
        "# INSTALL REQUIRED LIBRARIES\n",
        "# =============================================================================\n",
        "# openai: Python SDK for OpenAI's GPT models\n",
        "# anthropic: Python SDK for Anthropic's Claude models\n",
        "# transformers: HuggingFace library for local model loading\n",
        "# datasets: HuggingFace datasets library\n",
        "# accelerate: Optimized model loading for HuggingFace\n",
        "\n",
        "!pip install -q openai anthropic transformers datasets torch accelerate\n",
        "\n",
        "# =============================================================================\n",
        "# IMPORTS\n",
        "# =============================================================================\n",
        "import openai                              # OpenAI SDK\n",
        "from openai import OpenAI                  # OpenAI client class\n",
        "import anthropic                           # Anthropic SDK\n",
        "from anthropic import Anthropic            # Anthropic client class\n",
        "import torch                               # PyTorch (for HuggingFace models)\n",
        "from transformers import (\n",
        "    AutoTokenizer,                         # Load any tokenizer by name\n",
        "    AutoModelForSeq2SeqLM,                 # Encoder-decoder models (Flan-T5)\n",
        "    pipeline                               # High-level inference API\n",
        ")\n",
        "import numpy as np                         # Numerical operations\n",
        "import pandas as pd                        # Data manipulation\n",
        "import matplotlib.pyplot as plt            # Visualization\n",
        "import getpass                             # Secure password input\n",
        "import os                                  # Environment variables\n",
        "import json                                # JSON parsing\n",
        "import time                                # Rate limiting\n",
        "\n",
        "# =============================================================================\n",
        "# VERIFY INSTALLATIONS\n",
        "# =============================================================================\n",
        "print(\"Library versions:\")\n",
        "print(f\"  OpenAI SDK:    {openai.__version__}\")\n",
        "print(f\"  Anthropic SDK: {anthropic.__version__}\")\n",
        "print(f\"  PyTorch:       {torch.__version__}\")\n",
        "print(f\"  GPU available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  GPU device:    {torch.cuda.get_device_name(0)}\")\n",
        "print(\"\\n‚úÖ All libraries installed successfully!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/455.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m450.6/455.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m455.2/455.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLibrary versions:\n",
            "  OpenAI SDK:    2.23.0\n",
            "  Anthropic SDK: 0.84.0\n",
            "  PyTorch:       2.10.0+cu128\n",
            "  GPU available: True\n",
            "  GPU device:    Tesla T4\n",
            "\n",
            "‚úÖ All libraries installed successfully!\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-3",
        "outputId": "ce53a59a-d04c-4d82-9a63-d4e844253b20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# =============================================================================\n",
        "# API KEY SETUP\n",
        "# =============================================================================\n",
        "# Your instructor will provide temporary API keys for this session.\n",
        "# Keys are entered securely (not displayed) and stored only in memory.\n",
        "# They will be deactivated after class.\n",
        "\n",
        "print(\"Enter the API keys provided by your instructor:\")\n",
        "print(\"(Characters won't be displayed as you type)\\n\")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key: \")\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Anthropic API key: \")\n",
        "\n",
        "# Verify keys work with a quick test call\n",
        "print(\"\\nValidating keys...\")\n",
        "\n",
        "try:\n",
        "    test_client = OpenAI()\n",
        "    test_client.models.list()\n",
        "    print(\"‚úÖ OpenAI key validated\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå OpenAI key issue ‚Äî check with your instructor: {e}\")\n",
        "\n",
        "try:\n",
        "    test_client = Anthropic()\n",
        "    resp = test_client.messages.create(\n",
        "        model=\"claude-haiku-4-5-20251001\",\n",
        "        max_tokens=10,\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Say OK\"}]\n",
        "    )\n",
        "    print(\"‚úÖ Anthropic key validated\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Anthropic key issue ‚Äî check with your instructor: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the API keys provided by your instructor:\n",
            "(Characters won't be displayed as you type)\n",
            "\n",
            "OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Anthropic API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "\n",
            "Validating keys...\n",
            "‚úÖ OpenAI key validated\n",
            "‚ùå Anthropic key issue ‚Äî check with your instructor: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-4"
      },
      "source": [
        "# Section 1: What Are Large Language Models?\n",
        "\n",
        "You've watched the videos on transformers and attention. Let's quickly recap\n",
        "the key ideas and then focus on **why this matters for your work as data scientists**.\n",
        "\n",
        "## From Classical ML to LLMs\n",
        "\n",
        "In weeks 5-7, we built a fraud detection pipeline:\n",
        "- **Amazon Comprehend** analyzed sentiment and entities\n",
        "- We **engineered features** from those outputs\n",
        "- **XGBoost** learned to classify fraud vs legitimate\n",
        "\n",
        "That pipeline had a major limitation: **Comprehend gives us pre-defined features**\n",
        "(sentiment score, entity list). We couldn't ask it custom questions like:\n",
        "- *\"Does this transaction mention unusual timing?\"*\n",
        "- *\"What specific fraud indicators do you see?\"*\n",
        "- *\"Rate the risk level and explain your reasoning\"*\n",
        "\n",
        "**LLMs change this completely.** Instead of extracting fixed features, we can\n",
        "**ask the model any question in natural language** ‚Äî and get a natural language answer.\n",
        "\n",
        "## How Do LLMs Work? (The 60-Second Version)\n",
        "\n",
        "| Step | What Happens | Example |\n",
        "|------|-------------|--------|\n",
        "| **1. Tokenization** | Text is split into sub-word tokens | \"unauthorized\" ‚Üí [\"un\", \"authorized\"] |\n",
        "| **2. Attention** | Model looks at ALL tokens simultaneously, learns which are related | \"bank\" ‚Üî \"approved\" ‚Üî \"loan\" |\n",
        "| **3. Prediction** | Model predicts the next token, one at a time | World's best autocomplete |\n",
        "| **4. Scale** | Hundreds of billions of parameters, trained on trillions of words | GPT-4, Claude, Llama |\n",
        "\n",
        "**Key insight**: GPT-4, Claude, and Llama are all **decoder-only transformers**.\n",
        "They predict the next token given all previous tokens ‚Äî that's how they \"generate\" text.\n",
        "\n",
        "> üî¨ **Want to go deeper?** See the optional notebook\n",
        "> `week_11_optional_attention_transformers.ipynb` for hands-on attention visualization\n",
        "> and transformer architecture in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-5",
        "outputId": "f67d7260-b9b0-4b92-d8aa-077f5c7f553e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770,
          "referenced_widgets": [
            "62b63319e45e4ed5b6f1814f03d49c48",
            "226f14a2774c4a3a8d9052c054e19fae",
            "7b66a3493850474fbf1bc33f9216ae00",
            "8f884e0b7217410aabc7030c8d313285",
            "0b2198d04b4c441db0b125cb394f1ace",
            "4dfdbb62f9b942e4898bc51d6a68a577",
            "64e4f282c793498b90dbd04e87bc3700",
            "4730c47c251f4f1f92db492a4c3abc88",
            "04907b2f6af8454a9e92a311442cc5bb",
            "55a95112804749c6a91f45d66c8ea1f0",
            "abbf7ac1f3bf4d31b787323e6b0f1693",
            "87758d6a3e50487eb25172c990e39338",
            "c37a0be5b0324a23ac1c078f4463ac32",
            "df75788666884196b9fa137c07ea9c82",
            "75dda530eb9a4eb0963589fed3329c61",
            "5c0660ecd0e84501ab2d74311553871d",
            "904ac9d7310f4bb48441fd0cdda04c1f",
            "e02edfe82718409aaf3bc7f7ee22e8f2",
            "cbe96eee9cd74a98be8d58a46a170c3c",
            "b5c83a6a672a4368974c6dbe4e7d56d6",
            "1b328260c6914c2da5730beb6c95d7fe",
            "822a1461c9c3412a8e015e59ec3161d2",
            "b94a131413a04923b9207a452aa2a7b6",
            "3d7e05be56d64855a83dd3fa8c6f7609",
            "c415a1afd1854961b9c33a64713de0d4",
            "8815f5e29d2043bca3f2a16981485028",
            "97b75c05d70344fd834b65a5947aaab2",
            "24ebbf46154a42d2a86aeb0c30f3c756",
            "92aac135efb5408b9022fd6107b79d97",
            "4aaa4eab79954436adcb06d4f73ccd53",
            "195dac96e464449890a2474bcc65d83d",
            "a96d25be34494d00a3200687df8e1f09",
            "642b4b9b76d74aea83584d34bdf5efce",
            "205fd55701b74f2ba400ccab028e5fcb",
            "77af6a4d3da94b6d8e136f37eae068c7",
            "6411ecadded44bec9f95755996601b99",
            "a5ba10d75892494b86527c270a9a924c",
            "3fe9b16ef954441e85b8f7a14562a4a1",
            "4bede20ff5974dee8008f9a2e7e993a2",
            "ab87d3f40a88486599d16dfaae335108",
            "f9691c31b0ea44d483c852cbeefea567",
            "8a4bdcfd9a154d7bb58c5cbbca4c6f65",
            "1f835f3d57cb43aeb507020cbe43d92b",
            "dbbc9d13d1c5487aabc531f7bcb5ffe2",
            "1c0d50d490a54318bb819fa9eb923ce7",
            "ce3586969fb44c7692aa5958ab330d40",
            "f952fd29aaec47e69306d4df15ff00de",
            "191a3c6b84cb4a16a844e30caa003b5f",
            "aaf60fd6f9c044dd8a9e6dbb7b132b7c",
            "eeb63f4da8384105b5dd2823b5774ba2",
            "1e15ef71109b41239772cdd9027e2204",
            "3dbf303ab6ea4a488ab6215a251d0b9d",
            "3446d40830e044a8b5f660f120dd7126",
            "c6790e88c47b4c2ca2286d5813d00a43",
            "82554c5c3d3b4b0cbf6fd1e0d42e385e"
          ]
        }
      },
      "source": [
        "# =============================================================================\n",
        "# DEMO: How LLMs See Your Text ‚Äî Tokenization in Action\n",
        "# =============================================================================\n",
        "# Every LLM converts text to tokens (numbers) before processing.\n",
        "# Understanding tokenization helps you:\n",
        "#   - Estimate API costs (you pay per token!)\n",
        "#   - Understand context window limits\n",
        "#   - Debug unexpected model behavior\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Let's tokenize some fraud-related sentences\n",
        "texts = [\n",
        "    \"The customer reported unauthorized charges.\",\n",
        "    \"Monthly Netflix payment of $15.99.\",\n",
        "    \"Wire transfer to overseas account at 3 AM.\",\n",
        "]\n",
        "\n",
        "print(\"How GPT-2 tokenizes text:\\n\")\n",
        "for text in texts:\n",
        "    tokens = tokenizer.encode(text)\n",
        "    token_strings = tokenizer.convert_ids_to_tokens(tokens)\n",
        "\n",
        "    print(f\"Text: '{text}'\")\n",
        "    print(f\"  Tokens ({len(tokens)}): {token_strings}\")\n",
        "    print(f\"  Token IDs: {tokens}\")\n",
        "    print()\n",
        "\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\n",
        "print()\n",
        "print(\"üí° Key observations:\")\n",
        "print(\"  - Common words = single tokens ('The', 'customer')\")\n",
        "print(\"  - Rare words get split ('unauthorized' ‚Üí sub-word pieces)\")\n",
        "print(\"  - Numbers and punctuation = separate tokens\")\n",
        "print(\"  - The 'ƒ†' prefix means 'preceded by a space'\")\n",
        "print(\"  - More tokens = higher API cost!\")\n",
        "print()\n",
        "print(\"üåê Try it yourself! Paste any text at: https://tiktokenizer.vercel.app\")\n",
        "print(\"   You'll see exactly how different models tokenize your text.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62b63319e45e4ed5b6f1814f03d49c48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87758d6a3e50487eb25172c990e39338"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b94a131413a04923b9207a452aa2a7b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "205fd55701b74f2ba400ccab028e5fcb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c0d50d490a54318bb819fa9eb923ce7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How GPT-2 tokenizes text:\n",
            "\n",
            "Text: 'The customer reported unauthorized charges.'\n",
            "  Tokens (6): ['The', 'ƒ†customer', 'ƒ†reported', 'ƒ†unauthorized', 'ƒ†charges', '.']\n",
            "  Token IDs: [464, 6491, 2098, 22959, 4530, 13]\n",
            "\n",
            "Text: 'Monthly Netflix payment of $15.99.'\n",
            "  Tokens (10): ['Month', 'ly', 'ƒ†Netflix', 'ƒ†payment', 'ƒ†of', 'ƒ†$', '15', '.', '99', '.']\n",
            "  Token IDs: [31948, 306, 12074, 6074, 286, 720, 1314, 13, 2079, 13]\n",
            "\n",
            "Text: 'Wire transfer to overseas account at 3 AM.'\n",
            "  Tokens (9): ['Wire', 'ƒ†transfer', 'ƒ†to', 'ƒ†overseas', 'ƒ†account', 'ƒ†at', 'ƒ†3', 'ƒ†AM', '.']\n",
            "  Token IDs: [29451, 4351, 284, 11292, 1848, 379, 513, 3001, 13]\n",
            "\n",
            "Vocabulary size: 50,257 tokens\n",
            "\n",
            "üí° Key observations:\n",
            "  - Common words = single tokens ('The', 'customer')\n",
            "  - Rare words get split ('unauthorized' ‚Üí sub-word pieces)\n",
            "  - Numbers and punctuation = separate tokens\n",
            "  - The 'ƒ†' prefix means 'preceded by a space'\n",
            "  - More tokens = higher API cost!\n",
            "\n",
            "üåê Try it yourself! Paste any text at: https://tiktokenizer.vercel.app\n",
            "   You'll see exactly how different models tokenize your text.\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-6"
      },
      "source": [
        "## üí¨ Discussion 1: LLMs vs Traditional NLP (3-5 minutes)\n",
        "\n",
        "Discuss with a neighbor:\n",
        "\n",
        "1. **In Week 6, we used Amazon Comprehend** to extract sentiment and entities from\n",
        "   call transcripts. How is that different from what an LLM can do? What are the tradeoffs?\n",
        "\n",
        "2. **Cost vs capability**: Comprehend charges per API call for fixed outputs (sentiment,\n",
        "   entities). LLMs charge per token for flexible, custom outputs. When would you choose each?\n",
        "\n",
        "3. **Data privacy**: When you send transaction data to OpenAI or Anthropic, where does\n",
        "   that data go? What are the implications for a financial services company?\n",
        "   How does this compare to running a model locally with HuggingFace?\n",
        "\n",
        "> üè¶ Think about this from your company's perspective ‚Äî you handle sensitive\n",
        "> customer financial data subject to PCI-DSS (Payment Card Industry Data Security\n",
        "> Standard), SOX (Sarbanes-Oxley Act), and GDPR (General Data Protection Regulation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-7"
      },
      "source": [
        "# Section 2: Using LLMs with Cloud APIs (OpenAI & Anthropic)\n",
        "\n",
        "This is the most common way data scientists interact with LLMs in production today.\n",
        "You send a prompt, get a response ‚Äî no GPU, no model training, no infrastructure.\n",
        "\n",
        "## Three Ways to Use LLMs\n",
        "\n",
        "| Approach | Pros | Cons | When to Use |\n",
        "|----------|------|------|-------------|\n",
        "| **Cloud APIs** (OpenAI, Anthropic) | Easy, powerful, no GPU needed | Cost per call, data leaves your infra | Prototyping, low-volume, complex tasks |\n",
        "| **Local models** (HuggingFace) | Data stays local, no per-call cost | Need GPU, smaller models | Privacy-sensitive, high-volume |\n",
        "| **Fine-tuned models** | Best accuracy for your domain | Requires training data & compute | Domain-specific tasks (Week 14) |\n",
        "\n",
        "Today we'll use **Cloud APIs** (this section) and **HuggingFace** (Section 3).\n",
        "\n",
        "## Our Use Case: Fraud Detection with LLMs\n",
        "\n",
        "Remember our call center pipeline from weeks 5-7? Instead of training an XGBoost\n",
        "model from scratch, let's see how LLMs can analyze the same transaction descriptions\n",
        "using nothing but **natural language prompts**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-8",
        "outputId": "36211407-056b-41cb-ef2e-761716f463b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# =============================================================================\n",
        "# CREATE SYNTHETIC FRAUD TRANSACTION DATASET\n",
        "# =============================================================================\n",
        "# These descriptions simulate what a fraud analyst would review.\n",
        "# Each has clear signals that a human (or LLM) could use to classify.\n",
        "# We use synthetic data so we can share freely and know the ground truth.\n",
        "\n",
        "transaction_descriptions = [\n",
        "    {\n",
        "        \"id\": \"TXN-001\",\n",
        "        \"description\": \"Customer reports unauthorized wire transfer of $4,500 to unknown \"\n",
        "                       \"overseas account. No prior international transaction history. \"\n",
        "                       \"Transfer initiated at 3:47 AM local time.\",\n",
        "        \"actual_label\": \"fraud\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TXN-002\",\n",
        "        \"description\": \"Regular monthly payment of $89.99 to Netflix streaming service. \"\n",
        "                       \"Consistent with 18-month subscription history. Payment from \"\n",
        "                       \"primary checking account.\",\n",
        "        \"actual_label\": \"legitimate\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TXN-003\",\n",
        "        \"description\": \"Three consecutive ATM withdrawals totaling $1,500 in different \"\n",
        "                       \"cities within 2 hours. Card was reported lost the following day. \"\n",
        "                       \"Withdrawals at non-bank ATMs.\",\n",
        "        \"actual_label\": \"fraud\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TXN-004\",\n",
        "        \"description\": \"Online purchase of $234.56 at Amazon.com for household electronics. \"\n",
        "                       \"Shipping to address on file. Customer has frequent Amazon purchase \"\n",
        "                       \"history.\",\n",
        "        \"actual_label\": \"legitimate\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TXN-005\",\n",
        "        \"description\": \"Customer disputes charge of $2,100 at luxury jewelry store in Miami. \"\n",
        "                       \"Customer's location confirmed as Chicago at time of purchase. No \"\n",
        "                       \"travel alerts set.\",\n",
        "        \"actual_label\": \"fraud\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TXN-006\",\n",
        "        \"description\": \"Automatic payroll direct deposit of $3,245.67 from employer ABC Corp. \"\n",
        "                       \"Matches bi-weekly pay schedule. Amount consistent with employment \"\n",
        "                       \"records.\",\n",
        "        \"actual_label\": \"legitimate\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TXN-007\",\n",
        "        \"description\": \"Multiple small online purchases ($5-$15) at various digital stores \"\n",
        "                       \"within 30 minutes. None of these merchants appear in customer's \"\n",
        "                       \"history. Different IP addresses used.\",\n",
        "        \"actual_label\": \"fraud\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TXN-008\",\n",
        "        \"description\": \"Grocery purchase of $67.23 at Whole Foods Market. Customer shops \"\n",
        "                       \"here weekly based on 2-year transaction history. Paid with debit \"\n",
        "                       \"card at POS terminal.\",\n",
        "        \"actual_label\": \"legitimate\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Display the dataset\n",
        "df = pd.DataFrame(transaction_descriptions)\n",
        "print(f\"Created {len(df)} synthetic transaction descriptions\")\n",
        "print(f\"  Fraud cases:      {(df['actual_label'] == 'fraud').sum()}\")\n",
        "print(f\"  Legitimate cases:  {(df['actual_label'] == 'legitimate').sum()}\")\n",
        "print()\n",
        "for _, row in df.iterrows():\n",
        "    icon = \"üî¥\" if row['actual_label'] == 'fraud' else \"üü¢\"\n",
        "    print(f\"  {icon} [{row['id']}] {row['description'][:80]}...\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 8 synthetic transaction descriptions\n",
            "  Fraud cases:      4\n",
            "  Legitimate cases:  4\n",
            "\n",
            "  üî¥ [TXN-001] Customer reports unauthorized wire transfer of $4,500 to unknown overseas accoun...\n",
            "  üü¢ [TXN-002] Regular monthly payment of $89.99 to Netflix streaming service. Consistent with ...\n",
            "  üî¥ [TXN-003] Three consecutive ATM withdrawals totaling $1,500 in different cities within 2 h...\n",
            "  üü¢ [TXN-004] Online purchase of $234.56 at Amazon.com for household electronics. Shipping to ...\n",
            "  üî¥ [TXN-005] Customer disputes charge of $2,100 at luxury jewelry store in Miami. Customer's ...\n",
            "  üü¢ [TXN-006] Automatic payroll direct deposit of $3,245.67 from employer ABC Corp. Matches bi...\n",
            "  üî¥ [TXN-007] Multiple small online purchases ($5-$15) at various digital stores within 30 min...\n",
            "  üü¢ [TXN-008] Grocery purchase of $67.23 at Whole Foods Market. Customer shops here weekly bas...\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-9"
      },
      "source": [
        "## 2.1 Calling the OpenAI API\n",
        "\n",
        "The OpenAI Python SDK uses a **client pattern** ‚Äî create a client, then call methods:\n",
        "\n",
        "```python\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()  # Reads OPENAI_API_KEY from environment\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",           # Cost-efficient model (~$0.15/M input tokens)\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Your question here\"}\n",
        "    ],\n",
        "    temperature=0.0,               # 0 = deterministic, 1 = creative\n",
        "    max_tokens=500                  # Limit response length (and cost!)\n",
        ")\n",
        "\n",
        "# Extract the response text\n",
        "answer = response.choices[0].message.content\n",
        "```\n",
        "\n",
        "### Key Parameters\n",
        "\n",
        "| Parameter | What It Does | Typical Values |\n",
        "|-----------|-------------|----------------|\n",
        "| `model` | Which model to use | `\"gpt-4o-mini\"` (cheap), `\"gpt-4o\"` (powerful) |\n",
        "| `messages` | Conversation history | System prompt + user message |\n",
        "| `temperature` | Randomness control | `0.0` for classification, `0.7` for generation |\n",
        "| `max_tokens` | Response length limit | `20` for labels, `500` for analysis |\n",
        "\n",
        "### The Messages Format\n",
        "\n",
        "- **system**: Sets the AI's role, personality, and instructions\n",
        "- **user**: Your actual question or prompt\n",
        "- **assistant**: (Optional) Previous AI responses for multi-turn conversations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-10",
        "outputId": "7c9d0cee-28f0-43cc-cf09-3c5333cfa1b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# =============================================================================\n",
        "# DEMO: Zero-Shot Classification with OpenAI\n",
        "# =============================================================================\n",
        "# \"Zero-shot\" = we give instructions but NO examples.\n",
        "# The model relies entirely on its training to understand the task.\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Pick one transaction to classify\n",
        "sample = transaction_descriptions[0]  # Unauthorized wire transfer\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a fraud detection analyst at a financial institution. \"\n",
        "                \"Classify transaction descriptions as 'fraud' or 'legitimate'. \"\n",
        "                \"Respond with ONLY the classification label, nothing else.\"\n",
        "            )\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Classify this transaction:\\n\\n{sample['description']}\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.0,   # Deterministic ‚Äî same input always gives same output\n",
        "    max_tokens=20      # Short answer expected ‚Äî saves cost\n",
        ")\n",
        "\n",
        "# Extract and display results\n",
        "prediction = response.choices[0].message.content.strip().lower()\n",
        "actual = sample['actual_label']\n",
        "\n",
        "print(f\"Transaction: {sample['id']}\")\n",
        "print(f\"Description: {sample['description'][:100]}...\")\n",
        "print(f\"\\nPrediction:  {prediction}\")\n",
        "print(f\"Actual:      {actual}\")\n",
        "print(f\"Correct:     {'‚úÖ' if prediction == actual else '‚ùå'}\")\n",
        "print(f\"\\nToken usage:\")\n",
        "print(f\"  Input tokens:  {response.usage.prompt_tokens}\")\n",
        "print(f\"  Output tokens: {response.usage.completion_tokens}\")\n",
        "print(f\"  Total tokens:  {response.usage.total_tokens}\")\n",
        "print(f\"  Est. cost:     ${response.usage.total_tokens * 0.00000015:.6f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transaction: TXN-001\n",
            "Description: Customer reports unauthorized wire transfer of $4,500 to unknown overseas account. No prior internat...\n",
            "\n",
            "Prediction:  fraud\n",
            "Actual:      fraud\n",
            "Correct:     ‚úÖ\n",
            "\n",
            "Token usage:\n",
            "  Input tokens:  83\n",
            "  Output tokens: 2\n",
            "  Total tokens:  85\n",
            "  Est. cost:     $0.000013\n"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-11"
      },
      "source": [
        "## 2.2 Calling the Anthropic (Claude) API\n",
        "\n",
        "Anthropic's SDK is similar but has **key differences** you need to know:\n",
        "\n",
        "```python\n",
        "from anthropic import Anthropic\n",
        "\n",
        "client = Anthropic()  # Reads ANTHROPIC_API_KEY from environment\n",
        "\n",
        "message = client.messages.create(\n",
        "    model=\"claude-haiku-4-5-20251001\",     # Fast, cost-efficient model\n",
        "    max_tokens=500,\n",
        "    system=\"You are a helpful assistant.\",   # ‚ö†Ô∏è Separate parameter!\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Your question here\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Extract the response text\n",
        "answer = message.content[0].text           # ‚ö†Ô∏è Different from OpenAI!\n",
        "```\n",
        "\n",
        "### OpenAI vs Anthropic ‚Äî Quick Reference\n",
        "\n",
        "| Feature | OpenAI | Anthropic |\n",
        "|---------|--------|----------|\n",
        "| System prompt | Inside `messages` list | **Separate `system` parameter** |\n",
        "| Response text | `response.choices[0].message.content` | `message.content[0].text` |\n",
        "| Model (cheap) | `gpt-4o-mini` | `claude-haiku-4-5-20251001` |\n",
        "| Token count | `response.usage.total_tokens` | `message.usage.input_tokens + output_tokens` |\n",
        "| JSON mode | `response_format={\"type\": \"json_object\"}` | Instruct in prompt |\n",
        "\n",
        "These differences are small but will cause bugs if you mix them up!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-12",
        "outputId": "c492b575-70c3-4c03-f5b7-b585e1f5a0c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# =============================================================================\n",
        "# DEMO: Same Task with Anthropic Claude\n",
        "# =============================================================================\n",
        "# Same transaction, same prompt ‚Äî different provider.\n",
        "# Notice the API differences!\n",
        "\n",
        "client_anthropic = Anthropic()\n",
        "\n",
        "sample = transaction_descriptions[0]  # Same transaction as OpenAI demo\n",
        "\n",
        "message = client_anthropic.messages.create(\n",
        "    model=\"claude-haiku-4-5-20251001\",\n",
        "    max_tokens=20,\n",
        "    system=(                                          # ‚Üê system is a SEPARATE parameter\n",
        "        \"You are a fraud detection analyst at a financial institution. \"\n",
        "        \"Classify transaction descriptions as 'fraud' or 'legitimate'. \"\n",
        "        \"Respond with ONLY the classification label, nothing else.\"\n",
        "    ),\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Classify this transaction:\\n\\n{sample['description']}\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Extract results ‚Äî note different response structure!\n",
        "prediction = message.content[0].text.strip().lower()  # ‚Üê .content[0].text not .choices[0]...\n",
        "actual = sample['actual_label']\n",
        "\n",
        "print(f\"Transaction: {sample['id']}\")\n",
        "print(f\"Prediction:  {prediction}\")\n",
        "print(f\"Actual:      {actual}\")\n",
        "print(f\"Correct:     {'‚úÖ' if prediction == actual else '‚ùå'}\")\n",
        "print(f\"\\nToken usage:\")\n",
        "print(f\"  Input tokens:  {message.usage.input_tokens}\")\n",
        "print(f\"  Output tokens: {message.usage.output_tokens}\")\n",
        "print(f\"  Total tokens:  {message.usage.input_tokens + message.usage.output_tokens}\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "\"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-315/3328823448.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransaction_descriptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Same transaction as OpenAI demo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m message = client_anthropic.messages.create(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"claude-haiku-4-5-20251001\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/resources/messages/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, cache_control, container, inference_geo, metadata, output_config, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    994\u001b[0m             )\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m         )\n\u001b[0;32m-> 1364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0mremaining_retries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_retries\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mretries_taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1058\u001b[0;31m             \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries_taken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretries_taken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_build_request\u001b[0;34m(self, options, retries_taken)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected JSON data type, {type(json_data)}, cannot merge with `extra_body`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries_taken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretries_taken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_merge_mappings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mcontent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_build_headers\u001b[0;34m(self, options, retries_taken)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mcustom_headers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         )\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_headers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;31m# headers are case-insensitive while dictionaries are not.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/anthropic/_client.py\u001b[0m in \u001b[0;36m_validate_headers\u001b[0;34m(self, headers, custom_headers)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;34m'\"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         )\n",
            "\u001b[0;31mTypeError\u001b[0m: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\""
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-13"
      },
      "source": [
        "## üí¨ Discussion 2: Choosing an LLM Provider (3-5 minutes)\n",
        "\n",
        "You just saw the same task done with two different providers. Discuss:\n",
        "\n",
        "1. **Why would a company use multiple LLM providers** instead of just one?\n",
        "   Think about: reliability, cost negotiation, vendor lock-in, capability differences.\n",
        "\n",
        "2. **What happens when the API is down?** Our fraud detection pipeline needs to run\n",
        "   24/7. How would you design a fallback strategy? (Hint: think about what we did\n",
        "   with Transcribe fallbacks in Week 6.)\n",
        "\n",
        "3. **Reproducibility**: We set `temperature=0` for consistent results. But LLM\n",
        "   providers can update their models at any time. How does this affect a production\n",
        "   pipeline compared to a model you trained and version-controlled yourself?\n",
        "\n",
        "4. **Regulatory considerations**: In financial services, model decisions often need\n",
        "   to be **explainable and auditable**. How would you audit decisions made by an\n",
        "   external LLM vs your own XGBoost model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-14"
      },
      "source": [
        "## 2.3 Prompting Techniques\n",
        "\n",
        "The way you phrase your prompt **dramatically** affects the quality of LLM responses.\n",
        "Three fundamental techniques, from simplest to most powerful:\n",
        "\n",
        "### 1. Zero-Shot Prompting\n",
        "Give instructions only ‚Äî **no examples**. Works when the task is clear and unambiguous.\n",
        "\n",
        "```python\n",
        "\"Classify this transaction as fraud or legitimate: ...\"\n",
        "```\n",
        "\n",
        "### 2. Few-Shot Prompting\n",
        "Provide **2-5 examples** before your question. The examples teach the model your\n",
        "expected format and reasoning style through demonstration.\n",
        "\n",
        "```python\n",
        "\"Here are examples:\n",
        "Transaction: 'ATM withdrawal at 3am overseas' ‚Üí fraud\n",
        "Transaction: 'Monthly Netflix payment' ‚Üí legitimate\n",
        "\n",
        "Now classify this transaction: ...\"\n",
        "```\n",
        "\n",
        "### 3. Chain-of-Thought (CoT) Prompting\n",
        "Ask the model to **reason step-by-step** before giving a final answer. Best for\n",
        "complex tasks where you want to see (and audit) the reasoning.\n",
        "\n",
        "```python\n",
        "\"Analyze this transaction step by step:\n",
        "1. What is the transaction type?\n",
        "2. Are there any red flags?\n",
        "3. What is your confidence level?\n",
        "4. Final classification: fraud or legitimate\"\n",
        "```\n",
        "\n",
        "### Which to Use?\n",
        "\n",
        "| Technique | Speed | Cost | Quality | Best For |\n",
        "|-----------|-------|------|---------|----------|\n",
        "| Zero-shot | Fast | Cheap | Good | Simple, clear tasks |\n",
        "| Few-shot | Medium | Medium | Better | Consistent formatting |\n",
        "| Chain-of-thought | Slow | Expensive | Best | Complex reasoning, auditing |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-15"
      },
      "source": [
        "# =============================================================================\n",
        "# DEMO: Few-Shot Prompting ‚Äî Examples Improve Consistency\n",
        "# =============================================================================\n",
        "# By providing examples, we \"teach\" the model our expected format\n",
        "# and decision criteria through demonstration.\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# Build a few-shot prompt with 3 example classifications\n",
        "few_shot_prompt = \"\"\"Here are examples of transaction classifications:\n",
        "\n",
        "Example 1:\n",
        "Transaction: \"Unauthorized wire transfer of $3,200 to unknown overseas account at 2 AM\"\n",
        "Classification: fraud\n",
        "Reasoning: Unauthorized transfer, overseas destination, unusual time of day\n",
        "\n",
        "Example 2:\n",
        "Transaction: \"Monthly auto-payment of $150 for car insurance to State Farm\"\n",
        "Classification: legitimate\n",
        "Reasoning: Regular recurring payment, known insurance vendor, consistent amount\n",
        "\n",
        "Example 3:\n",
        "Transaction: \"Five rapid purchases at different gas stations across 3 states in one hour\"\n",
        "Classification: fraud\n",
        "Reasoning: Physically impossible travel pattern, rapid successive small purchases\n",
        "\n",
        "Now classify this transaction:\n",
        "Transaction: \"{description}\"\n",
        "\n",
        "Provide your answer in this format:\n",
        "Classification: <fraud or legitimate>\n",
        "Reasoning: <brief explanation of key signals>\"\"\"\n",
        "\n",
        "# Test on the jewelry store dispute (TXN-005)\n",
        "sample = transaction_descriptions[4]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert fraud analyst at a bank.\"},\n",
        "        {\"role\": \"user\", \"content\": few_shot_prompt.format(description=sample['description'])}\n",
        "    ],\n",
        "    temperature=0.0,\n",
        "    max_tokens=100\n",
        ")\n",
        "\n",
        "print(f\"Transaction {sample['id']}:\")\n",
        "print(f\"  {sample['description'][:80]}...\")\n",
        "print(f\"\\nModel Response:\")\n",
        "print(f\"  {response.choices[0].message.content}\")\n",
        "print(f\"\\nActual label: {sample['actual_label']}\")\n",
        "print(f\"\\nüí° Notice: The model follows the EXACT format from our examples!\")\n",
        "print(f\"   Few-shot prompting is powerful for controlling output structure.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-16"
      },
      "source": [
        "# =============================================================================\n",
        "# DEMO: Chain-of-Thought ‚Äî The Model Shows Its Reasoning\n",
        "# =============================================================================\n",
        "# CoT is especially useful when you need to AUDIT the decision.\n",
        "# The model explains WHY it classified something a certain way.\n",
        "\n",
        "client_anthropic = Anthropic()\n",
        "\n",
        "sample = transaction_descriptions[6]  # Multiple small online purchases\n",
        "\n",
        "message = client_anthropic.messages.create(\n",
        "    model=\"claude-haiku-4-5-20251001\",\n",
        "    max_tokens=400,\n",
        "    system=(\n",
        "        \"You are a senior fraud analyst at a financial institution. \"\n",
        "        \"When analyzing transactions, think step by step:\\n\"\n",
        "        \"1. Identify the transaction type and amount pattern\\n\"\n",
        "        \"2. Check for red flags (unusual time, location, pattern, velocity)\\n\"\n",
        "        \"3. Compare against typical customer behavior described\\n\"\n",
        "        \"4. Assess confidence level (low/medium/high)\\n\"\n",
        "        \"5. Provide final classification (fraud or legitimate)\"\n",
        "    ),\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Analyze this transaction step by step:\\n\\n{sample['description']}\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"Transaction {sample['id']}:\")\n",
        "print(f\"Description: {sample['description']}\")\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Chain-of-Thought Analysis:\\n\")\n",
        "print(message.content[0].text)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Actual label: {sample['actual_label']}\")\n",
        "print(f\"\\nüí° CoT gives us an AUDIT TRAIL ‚Äî crucial for regulated industries!\")\n",
        "print(f\"   A compliance officer can review WHY the model flagged this transaction.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-17"
      },
      "source": [
        "## üí¨ Discussion 3: Prompting Consequences (3-5 minutes)\n",
        "\n",
        "You've seen three prompting strategies produce very different outputs. Discuss:\n",
        "\n",
        "1. **Zero-shot gave us a label. Few-shot gave us a label + reasoning. Chain-of-thought\n",
        "   gave us a full analysis.** Which would you use in a production fraud pipeline?\n",
        "   Consider: speed, cost, auditability, and regulatory requirements.\n",
        "\n",
        "2. **The few-shot examples we chose shape the model's behavior.** What happens if our\n",
        "   examples are biased? (e.g., all fraud examples involve international transactions\n",
        "   ‚Äî would the model learn \"international = fraud\"?) How is this different from\n",
        "   training data bias in traditional ML?\n",
        "\n",
        "3. **Chain-of-thought reveals reasoning ‚Äî but can we trust it?** The model might give\n",
        "   a confident explanation for a wrong answer. This is called \"hallucinated reasoning.\"\n",
        "   How would you validate LLM outputs in a real fraud detection pipeline?\n",
        "\n",
        "4. **Who writes the prompts?** In production, prompt engineering becomes a critical\n",
        "   skill. Should data scientists own prompts, or should it be a separate role?\n",
        "   What governance and version control is needed for prompts?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-18"
      },
      "source": [
        "## Lab 1: Classify Transactions with OpenAI & Anthropic (15 minutes)\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Build classification functions for **both providers** using few-shot prompting,\n",
        "run them on all 8 transactions, and compare accuracy.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Create `classify_with_openai(description)`** ‚Äî use the few-shot prompt template\n",
        "   from the demo to classify a transaction with GPT-4o-mini\n",
        "2. **Create `classify_with_anthropic(description)`** ‚Äî same task with Claude Haiku\n",
        "3. **Loop through all 8 transactions** and classify with both providers\n",
        "4. **Create a results DataFrame** and calculate accuracy for each provider\n",
        "\n",
        "### Expected Output\n",
        "\n",
        "A DataFrame with columns: `id`, `actual_label`, `openai_prediction`, `anthropic_prediction`\n",
        "Plus accuracy percentages for each provider.\n",
        "\n",
        "### Hints\n",
        "\n",
        "- Use `temperature=0.0` for consistent results\n",
        "- Add `time.sleep(0.5)` between API calls to avoid rate limits\n",
        "- Parse the response to extract just 'fraud' or 'legitimate' (use `.strip().lower()`)\n",
        "- The `FEW_SHOT_EXAMPLES` string is provided as a starting point\n",
        "\n",
        "### üìù Homework Extension\n",
        "\n",
        "After class: modify the system prompt to also return a **confidence score (0-100)**.\n",
        "Does adding confidence change the classification quality? How consistent are the\n",
        "confidence scores across runs?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-19"
      },
      "source": [
        "# =============================================================================\n",
        "# LAB 1: CLASSIFY TRANSACTIONS WITH OPENAI & ANTHROPIC\n",
        "# =============================================================================\n",
        "\n",
        "# Few-shot examples to include in your prompts\n",
        "FEW_SHOT_EXAMPLES = \"\"\"\n",
        "Example 1:\n",
        "Transaction: \"Unauthorized wire transfer of $3,200 to unknown overseas account at 2 AM\"\n",
        "Classification: fraud\n",
        "\n",
        "Example 2:\n",
        "Transaction: \"Monthly auto-payment of $150 for car insurance to State Farm\"\n",
        "Classification: legitimate\n",
        "\n",
        "Example 3:\n",
        "Transaction: \"Five rapid purchases at different gas stations across 3 states in one hour\"\n",
        "Classification: fraud\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def classify_with_openai(description):\n",
        "    \"\"\"\n",
        "    Classify a single transaction description using OpenAI GPT-4o-mini\n",
        "    with few-shot prompting. Returns 'fraud' or 'legitimate'.\n",
        "    \"\"\"\n",
        "    client = OpenAI()\n",
        "\n",
        "    # Build the prompt using FEW_SHOT_EXAMPLES + the transaction description\n",
        "    user_prompt = f\"\"\"{FEW_SHOT_EXAMPLES}\n",
        "Now classify this transaction (respond with ONLY 'fraud' or 'legitimate'):\n",
        "Transaction: \"{description}\"\n",
        "Classification:\"\"\"\n",
        "\n",
        "    # YOUR CODE: Create the chat completion request\n",
        "    response = None  # YOUR CODE\n",
        "\n",
        "    # YOUR CODE: Extract and clean the prediction text\n",
        "    prediction = None  # YOUR CODE\n",
        "\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def classify_with_anthropic(description):\n",
        "    \"\"\"\n",
        "    Classify a single transaction description using Anthropic Claude Haiku\n",
        "    with few-shot prompting. Returns 'fraud' or 'legitimate'.\n",
        "    \"\"\"\n",
        "    client_anthropic = Anthropic()\n",
        "\n",
        "    user_prompt = f\"\"\"{FEW_SHOT_EXAMPLES}\n",
        "Now classify this transaction (respond with ONLY 'fraud' or 'legitimate'):\n",
        "Transaction: \"{description}\"\n",
        "Classification:\"\"\"\n",
        "\n",
        "    # YOUR CODE: Create the message request\n",
        "    message = None  # YOUR CODE\n",
        "\n",
        "    # YOUR CODE: Extract and clean the prediction text\n",
        "    prediction = None  # YOUR CODE\n",
        "\n",
        "    return prediction\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# RUN CLASSIFICATION ON ALL TRANSACTIONS\n",
        "# =============================================================================\n",
        "results = []\n",
        "\n",
        "for txn in transaction_descriptions:\n",
        "    print(f\"Classifying {txn['id']}...\")\n",
        "\n",
        "    # YOUR CODE: Call both classifiers\n",
        "    openai_pred = None  # YOUR CODE\n",
        "    time.sleep(0.5)  # Rate limiting\n",
        "\n",
        "    anthropic_pred = None  # YOUR CODE\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    results.append({\n",
        "        'id': txn['id'],\n",
        "        'actual_label': txn['actual_label'],\n",
        "        'openai_prediction': openai_pred,\n",
        "        'anthropic_prediction': anthropic_pred\n",
        "    })\n",
        "\n",
        "# YOUR CODE: Create DataFrame\n",
        "results_df = None  # YOUR CODE\n",
        "\n",
        "# YOUR CODE: Calculate accuracy for each provider\n",
        "openai_accuracy = None  # YOUR CODE\n",
        "anthropic_accuracy = None  # YOUR CODE\n",
        "\n",
        "# Verification\n",
        "if results_df is not None:\n",
        "    print(f\"\\n‚úÖ Classification complete!\")\n",
        "    display(results_df)\n",
        "    print(f\"\\nOpenAI Accuracy:    {openai_accuracy:.1%}\")\n",
        "    print(f\"Anthropic Accuracy: {anthropic_accuracy:.1%}\")\n",
        "else:\n",
        "    print(\"‚ùå Lab 1 incomplete ‚Äî fill in the YOUR CODE sections above\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-20"
      },
      "source": [
        "## 2.4 Beyond Classification: Structured Data Extraction\n",
        "\n",
        "Classification gives us a label. But LLMs can do much more ‚Äî they can extract\n",
        "**structured data** from unstructured text, returning JSON with exactly the\n",
        "fields you need.\n",
        "\n",
        "This would traditionally require:\n",
        "- Custom NER (Named Entity Recognition) models\n",
        "- Complex regex patterns\n",
        "- Multiple Comprehend API calls + post-processing\n",
        "\n",
        "With an LLM, we just **describe the output format we want**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-21"
      },
      "source": [
        "# =============================================================================\n",
        "# DEMO: Extract Structured Fraud Signals as JSON\n",
        "# =============================================================================\n",
        "# Instead of just \"fraud/legitimate\", we ask for a complete risk assessment.\n",
        "# OpenAI supports response_format={\"type\": \"json_object\"} to enforce valid JSON.\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "sample = transaction_descriptions[2]  # ATM withdrawals\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\"\"You are a fraud analyst. Analyze transactions and return a JSON object:\n",
        "{\n",
        "    \"risk_level\": \"low|medium|high|critical\",\n",
        "    \"red_flags\": [\"list of specific suspicious indicators\"],\n",
        "    \"recommended_action\": \"approve|review|block|escalate\",\n",
        "    \"confidence\": 0-100,\n",
        "    \"reasoning\": \"brief explanation\"\n",
        "}\n",
        "Return ONLY valid JSON, no other text.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Analyze this transaction:\\n\\n{sample['description']}\"\n",
        "        }\n",
        "    ],\n",
        "    temperature=0.0,\n",
        "    max_tokens=300,\n",
        "    response_format={\"type\": \"json_object\"}  # Enforces valid JSON output\n",
        ")\n",
        "\n",
        "# Parse the JSON response\n",
        "raw = response.choices[0].message.content\n",
        "result = json.loads(raw)\n",
        "\n",
        "print(f\"Transaction: {sample['id']}\")\n",
        "print(f\"Description: {sample['description'][:80]}...\")\n",
        "print(f\"\\nExtracted Fraud Signals:\")\n",
        "print(f\"  Risk Level:     {result['risk_level']}\")\n",
        "print(f\"  Red Flags:      {result['red_flags']}\")\n",
        "print(f\"  Action:         {result['recommended_action']}\")\n",
        "print(f\"  Confidence:     {result['confidence']}%\")\n",
        "print(f\"  Reasoning:      {result['reasoning']}\")\n",
        "print(f\"\\nüí° Compare this to Comprehend from Week 5-6:\")\n",
        "print(f\"   Comprehend gave us: sentiment score, entity list, key phrases\")\n",
        "print(f\"   LLM gave us: custom fields with risk assessment and reasoning!\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-22"
      },
      "source": [
        "# =============================================================================\n",
        "# DEMO: Batch Extraction Across All Transactions\n",
        "# =============================================================================\n",
        "# Run the structured extraction on all 8 transactions and visualize results.\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "system_prompt = \"\"\"You are a fraud analyst. Analyze transactions and return JSON:\n",
        "{\n",
        "    \"risk_level\": \"low|medium|high|critical\",\n",
        "    \"red_flags\": [\"list\"],\n",
        "    \"recommended_action\": \"approve|review|block|escalate\",\n",
        "    \"confidence\": 0-100\n",
        "}\n",
        "Return ONLY valid JSON.\"\"\"\n",
        "\n",
        "all_signals = []\n",
        "\n",
        "print(\"Extracting fraud signals from all transactions...\")\n",
        "for txn in transaction_descriptions:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": f\"Analyze:\\n\\n{txn['description']}\"}\n",
        "        ],\n",
        "        temperature=0.0,\n",
        "        max_tokens=200,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    signals = json.loads(response.choices[0].message.content)\n",
        "    signals['id'] = txn['id']\n",
        "    signals['actual_label'] = txn['actual_label']\n",
        "    all_signals.append(signals)\n",
        "    time.sleep(0.3)\n",
        "    print(f\"  ‚úì {txn['id']}: {signals['risk_level']} ({signals['confidence']}%)\")\n",
        "\n",
        "# Create DataFrame and display\n",
        "signals_df = pd.DataFrame(all_signals)[\n",
        "    ['id', 'actual_label', 'risk_level', 'recommended_action', 'confidence']\n",
        "]\n",
        "print(f\"\\n{'='*60}\")\n",
        "display(signals_df)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Risk level distribution\n",
        "risk_order = ['low', 'medium', 'high', 'critical']\n",
        "risk_counts = signals_df['risk_level'].value_counts().reindex(risk_order, fill_value=0)\n",
        "colors = {'low': '#2ecc71', 'medium': '#f39c12', 'high': '#e74c3c', 'critical': '#8e44ad'}\n",
        "risk_counts.plot(kind='bar', ax=axes[0], color=[colors.get(r, 'gray') for r in risk_counts.index])\n",
        "axes[0].set_title('Risk Level Distribution')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=0)\n",
        "\n",
        "# Confidence by actual label\n",
        "signals_df.groupby('actual_label')['confidence'].mean().plot(\n",
        "    kind='bar', ax=axes[1], color=['#2ecc71', '#e74c3c']\n",
        ")\n",
        "axes[1].set_title('Avg Confidence by Actual Label')\n",
        "axes[1].set_ylabel('Confidence %')\n",
        "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° The LLM correctly assigned higher risk to fraud transactions!\")\n",
        "print(\"   And it did this with ZERO training ‚Äî just a well-crafted prompt.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-23"
      },
      "source": [
        "## üí¨ Discussion 4: LLMs in Production Pipelines (3-5 minutes)\n",
        "\n",
        "Looking at the structured extraction results above:\n",
        "\n",
        "1. **The LLM effectively replaced Comprehend + feature engineering + XGBoost** with a\n",
        "   single API call. Is this always better? What are the risks of replacing a trained,\n",
        "   validated model with prompt-based classification?\n",
        "\n",
        "2. **Consistency**: If you run the extraction twice, do you get the exact same confidence\n",
        "   scores? What does this mean for reproducibility and regression testing?\n",
        "\n",
        "3. **Cost at scale**: Our fraud pipeline processes thousands of transactions daily.\n",
        "   At ~500 tokens/call and $0.15/million input tokens (GPT-4o-mini), what's the daily\n",
        "   cost for 10,000 transactions? At what volume does it make sense to train a custom model?\n",
        "\n",
        "4. **The \"black box\" problem**: XGBoost gives feature importance scores. An LLM gives\n",
        "   reasoning text. Which is more trustworthy for regulatory compliance? Which is more\n",
        "   useful for a human fraud analyst reviewing the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-24"
      },
      "source": [
        "# Section 3: Local Text Generation with HuggingFace\n",
        "\n",
        "Cloud APIs are powerful, but they have tradeoffs: cost per call, data leaves your\n",
        "infrastructure, and you depend on an external service. **HuggingFace Transformers**\n",
        "gives you direct access to open-source models you can run locally.\n",
        "\n",
        "## Key HuggingFace Concepts\n",
        "\n",
        "| Concept | What It Does | Example |\n",
        "|---------|-------------|--------|\n",
        "| **AutoTokenizer** | Converts text ‚Üî token IDs | `AutoTokenizer.from_pretrained(\"gpt2\")` |\n",
        "| **pipeline()** | Easiest way to run a model | `pipeline(\"text-generation\", model=\"gpt2\")` |\n",
        "| **AutoModelForSeq2SeqLM** | Encoder-decoder models | Flan-T5 (follows instructions) |\n",
        "\n",
        "## Why Learn This?\n",
        "\n",
        "Even if you mostly use cloud APIs, HuggingFace is essential for:\n",
        "- **Fine-tuning** models on your own data (coming in Week 14)\n",
        "- **Running models locally** ‚Äî your data never leaves your machine\n",
        "- **Evaluating open-source alternatives** to paid APIs\n",
        "- **Understanding tokenization** ‚Äî the foundation of all LLM interaction and costs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-25"
      },
      "source": [
        "# =============================================================================\n",
        "# DEMO: HuggingFace Pipeline ‚Äî The Easiest Way to Generate Text\n",
        "# =============================================================================\n",
        "# The pipeline() API wraps tokenization + model + decoding in one call.\n",
        "# It works the same way for ANY text generation model.\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"gpt2\",      # 124M parameters ‚Äî small but instructive\n",
        "    device=-1           # -1 = CPU, 0 = first GPU\n",
        ")\n",
        "\n",
        "# Generate text continuation\n",
        "prompt = \"The fraud analyst reviewed the transaction and determined that\"\n",
        "\n",
        "outputs = generator(\n",
        "    prompt,\n",
        "    max_new_tokens=50,       # Generate up to 50 new tokens\n",
        "    temperature=0.7,         # Some creativity\n",
        "    do_sample=True,          # Enable sampling (not greedy)\n",
        "    num_return_sequences=2   # Generate 2 different completions\n",
        ")\n",
        "\n",
        "print(f\"Prompt: '{prompt}'\\n\")\n",
        "for i, output in enumerate(outputs):\n",
        "    print(f\"Completion {i+1}:\")\n",
        "    print(f\"  {output['generated_text']}\")\n",
        "    print()\n",
        "\n",
        "print(\"üí° GPT-2 is small (124M params) ‚Äî quality is limited!\")\n",
        "print(\"   But the pipeline API works identically for larger models.\")\n",
        "print(\"   Swap 'gpt2' for 'meta-llama/Llama-3-8B' and you get much better output.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-26"
      },
      "source": [
        "# =============================================================================\n",
        "# DEMO: Flan-T5 ‚Äî A Model That Follows Instructions\n",
        "# =============================================================================\n",
        "# Flan-T5 is an encoder-decoder model fine-tuned on 1,000+ tasks.\n",
        "# Unlike GPT-2 (which just continues text), Flan-T5 understands\n",
        "# task prompts like \"classify\", \"summarize\", \"translate\".\n",
        "\n",
        "flan_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "flan_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "\n",
        "def generate_with_flan(prompt, max_new_tokens=100):\n",
        "    \"\"\"Generate a response from Flan-T5 given a task prompt.\"\"\"\n",
        "    inputs = flan_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = flan_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False  # Greedy decoding ‚Äî deterministic output\n",
        "    )\n",
        "    return flan_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# === Test 1: Zero-shot classification ===\n",
        "result = generate_with_flan(\n",
        "    \"Classify the following as fraud or legitimate: \"\n",
        "    \"'Three ATM withdrawals in different cities within one hour'\"\n",
        ")\n",
        "print(f\"Zero-shot classification: {result}\")\n",
        "\n",
        "# === Test 2: One-shot classification ===\n",
        "result = generate_with_flan(\n",
        "    \"\"\"Classify the transaction:\n",
        "\n",
        "Example:\n",
        "Transaction: \"Monthly Netflix payment of $15.99\"\n",
        "Classification: legitimate\n",
        "\n",
        "Transaction: \"Five purchases at gas stations across 3 states in one hour\"\n",
        "Classification:\"\"\"\n",
        ")\n",
        "print(f\"One-shot classification:  {result}\")\n",
        "\n",
        "# === Test 3: Sentiment analysis ===\n",
        "result = generate_with_flan(\n",
        "    \"What is the sentiment of: \"\n",
        "    \"'I am extremely upset about the unauthorized charges on my account'\"\n",
        ")\n",
        "print(f\"Sentiment analysis:      {result}\")\n",
        "\n",
        "print(f\"\\nüí° Flan-T5 understands task instructions!\")\n",
        "print(f\"   But it's 250M params vs GPT-4's ~1 trillion ‚Äî quality reflects that.\")\n",
        "print(f\"   Great for learning the API; use larger models for production.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-27"
      },
      "source": [
        "## üí¨ Discussion 5: Cloud vs Local Models (3-5 minutes)\n",
        "\n",
        "You've now seen both cloud APIs and local HuggingFace models in action. Discuss:\n",
        "\n",
        "1. **Quality gap**: GPT-4o-mini and Claude Haiku gave excellent fraud classifications.\n",
        "   Flan-T5 gave reasonable but less nuanced results. When is \"good enough\" actually\n",
        "   good enough? What accuracy threshold would a fraud team accept?\n",
        "\n",
        "2. **Data sovereignty**: Financial regulations (GDPR, SOX, PCI-DSS) may restrict where\n",
        "   customer data can be sent. How does this affect your choice of cloud vs local?\n",
        "   Can you use cloud APIs if you anonymize the data first?\n",
        "\n",
        "3. **Latency and throughput**: Cloud APIs have ~500ms network latency. Local models have\n",
        "   load time but sub-100ms inference. Which matters more for real-time fraud detection\n",
        "   vs nightly batch processing?\n",
        "\n",
        "4. **Open source risk and reward**: HuggingFace models are community-maintained. What\n",
        "   are the risks of using an open-source model in production (license changes,\n",
        "   vulnerabilities, quality regression)? How do you vet a model before deploying it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-28"
      },
      "source": [
        "## Lab 2: Prompting Workshop with HuggingFace (15 minutes)\n",
        "\n",
        "### Your Task\n",
        "\n",
        "Compare **zero-shot, one-shot, and few-shot prompting** with Flan-T5 on our\n",
        "fraud classification task. See how adding examples affects a smaller model.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. Write three prompt templates (zero-shot, one-shot, few-shot with 3 examples)\n",
        "2. Run each strategy on 4 test transactions (TXN-001, TXN-003, TXN-005, TXN-007)\n",
        "3. Record predictions and calculate accuracy per strategy\n",
        "4. Observe: How does adding examples change the results?\n",
        "\n",
        "### Expected Output\n",
        "\n",
        "A DataFrame comparing predictions across strategies, plus accuracy per strategy.\n",
        "\n",
        "### Hints\n",
        "\n",
        "- Use the `generate_with_flan()` function from the demo above\n",
        "- Keep prompts under 512 tokens (Flan-T5-base context limit)\n",
        "- Normalize responses: `.strip().lower()` ‚Äî model may return 'Fraud' vs 'fraud'\n",
        "- Use DIFFERENT examples than the test transactions!\n",
        "\n",
        "### üìù Homework Extension\n",
        "\n",
        "After class: repeat this experiment using **OpenAI or Anthropic** instead of Flan-T5.\n",
        "How do prompting techniques compare across model sizes? Also try writing a\n",
        "chain-of-thought prompt for Flan-T5 ‚Äî can a smaller model reason step by step?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cell-29"
      },
      "source": [
        "# =============================================================================\n",
        "# LAB 2: PROMPTING WORKSHOP WITH HUGGINGFACE\n",
        "# =============================================================================\n",
        "\n",
        "# Test on 4 transactions (mix of fraud and legitimate)\n",
        "test_transactions = [transaction_descriptions[i] for i in [0, 2, 4, 6]]\n",
        "\n",
        "\n",
        "def classify_zero_shot(description):\n",
        "    \"\"\"Zero-shot classification with Flan-T5 ‚Äî instructions only, no examples.\"\"\"\n",
        "    # YOUR CODE: Create a zero-shot prompt for fraud classification\n",
        "    prompt = None  # YOUR CODE\n",
        "\n",
        "    result = None  # YOUR CODE\n",
        "    return result\n",
        "\n",
        "\n",
        "def classify_one_shot(description):\n",
        "    \"\"\"One-shot classification with Flan-T5 ‚Äî one example provided.\"\"\"\n",
        "    # YOUR CODE: Create a one-shot prompt with 1 example + the transaction\n",
        "    prompt = None  # YOUR CODE\n",
        "\n",
        "    result = None  # YOUR CODE\n",
        "    return result\n",
        "\n",
        "\n",
        "def classify_few_shot(description):\n",
        "    \"\"\"Few-shot classification with Flan-T5 ‚Äî three examples provided.\"\"\"\n",
        "    # YOUR CODE: Create a few-shot prompt with 3 examples + the transaction\n",
        "    # Use different examples than the test transactions!\n",
        "    prompt = None  # YOUR CODE\n",
        "\n",
        "    result = None  # YOUR CODE\n",
        "    return result\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# RUN ALL STRATEGIES\n",
        "# =============================================================================\n",
        "strategies = {\n",
        "    'zero_shot': classify_zero_shot,\n",
        "    'one_shot': classify_one_shot,\n",
        "    'few_shot': classify_few_shot\n",
        "}\n",
        "\n",
        "lab2_results = []\n",
        "\n",
        "for txn in test_transactions:\n",
        "    row = {'id': txn['id'], 'actual': txn['actual_label']}\n",
        "\n",
        "    for strategy_name, classify_fn in strategies.items():\n",
        "        # YOUR CODE: Call the classification function\n",
        "        prediction = None  # YOUR CODE\n",
        "        row[strategy_name] = prediction\n",
        "\n",
        "    lab2_results.append(row)\n",
        "    print(f\"Classified {txn['id']}: {row}\")\n",
        "\n",
        "# YOUR CODE: Create DataFrame and calculate accuracy for each strategy\n",
        "lab2_df = None  # YOUR CODE\n",
        "\n",
        "zero_shot_acc = None  # YOUR CODE\n",
        "one_shot_acc = None   # YOUR CODE\n",
        "few_shot_acc = None   # YOUR CODE\n",
        "\n",
        "# Verification\n",
        "if lab2_df is not None:\n",
        "    print(f\"\\n‚úÖ Prompting Workshop Results:\")\n",
        "    display(lab2_df)\n",
        "    print(f\"\\nAccuracy by Strategy:\")\n",
        "    print(f\"  Zero-shot: {zero_shot_acc:.1%}\")\n",
        "    print(f\"  One-shot:  {one_shot_acc:.1%}\")\n",
        "    print(f\"  Few-shot:  {few_shot_acc:.1%}\")\n",
        "    print(f\"\\nüí° How did adding examples affect the smaller model?\")\n",
        "else:\n",
        "    print(\"‚ùå Lab 2 incomplete ‚Äî fill in the YOUR CODE sections above\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-30"
      },
      "source": [
        "# Summary: What We Learned Today\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "### LLMs vs Traditional ML\n",
        "- LLMs perform classification, extraction, and analysis with **natural language prompts**\n",
        "- No model training needed ‚Äî but tradeoffs in cost, consistency, and control\n",
        "- Traditional ML (XGBoost) gives reproducibility and feature importance\n",
        "- LLMs give flexibility, natural language reasoning, and structured extraction\n",
        "\n",
        "### Cloud APIs (OpenAI & Anthropic)\n",
        "| | OpenAI | Anthropic |\n",
        "|---|--------|----------|\n",
        "| **Create request** | `client.chat.completions.create()` | `client.messages.create()` |\n",
        "| **Get response** | `response.choices[0].message.content` | `message.content[0].text` |\n",
        "| **System prompt** | Inside `messages` list | Separate `system` parameter |\n",
        "| **Model used** | `gpt-4o-mini` | `claude-haiku-4-5-20251001` |\n",
        "\n",
        "### Prompting Techniques\n",
        "| Technique | Best For | Tradeoff |\n",
        "|-----------|----------|----------|\n",
        "| **Zero-shot** | Simple, clear tasks | Fast & cheap, less consistent |\n",
        "| **Few-shot** | Consistent formatting | Better results, costs more tokens |\n",
        "| **Chain-of-thought** | Complex reasoning & auditing | Best analysis, most expensive |\n",
        "\n",
        "### HuggingFace\n",
        "- Run models locally ‚Äî data never leaves your machine\n",
        "- `pipeline()` is the easiest entry point for any task\n",
        "- Flan-T5 follows instructions; GPT-2 does text completion\n",
        "- Smaller models = less capable but free, private, and fast\n",
        "\n",
        "## The Bigger Picture\n",
        "\n",
        "| Week | Approach | Tool |\n",
        "|------|----------|------|\n",
        "| 5-6 | Pre-trained NLP services | Amazon Comprehend |\n",
        "| 6 | Traditional ML | SageMaker XGBoost |\n",
        "| **11** | **LLM prompting** | **OpenAI, Anthropic, HuggingFace** |\n",
        "| 12+ | GenAI applications | Building on today's foundations |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-31"
      },
      "source": [
        "# Homework & Optional Labs\n",
        "\n",
        "## üìù Homework (Complete before next session)\n",
        "\n",
        "### Homework 1: Structured Extraction with Anthropic\n",
        "Using the JSON extraction demo (Section 2.4: Structured Data Extraction) as a\n",
        "template, write a function that extracts fraud signals from all 8 transactions\n",
        "using **Anthropic Claude** instead of OpenAI. Compare the JSON outputs:\n",
        "- Do the providers flag the same red flags?\n",
        "- Are confidence scores similar?\n",
        "- Which gives more actionable reasoning?\n",
        "\n",
        "### Homework 2: Prompt Optimization Journey\n",
        "Start with a basic zero-shot prompt and iteratively improve it through 4 stages:\n",
        "1. Zero-shot (baseline accuracy)\n",
        "2. Add few-shot examples (measure accuracy change)\n",
        "3. Add chain-of-thought instructions (measure accuracy change)\n",
        "4. Add output format constraints (measure accuracy change)\n",
        "\n",
        "Create a table showing accuracy at each stage. Write 2-3 sentences about what\n",
        "you learned about prompt design.\n",
        "\n",
        "## üî¨ Optional Labs (For advanced learners)\n",
        "\n",
        "### Optional Lab 1: Temperature Exploration\n",
        "Run the same classification prompt 5 times each at temperatures 0.0, 0.3, 0.7, 1.0.\n",
        "Plot the variance in results across runs. What does this tell you about choosing\n",
        "temperature settings for production fraud detection?\n",
        "\n",
        "### Optional Lab 2: Model Comparison Dashboard\n",
        "Classify all 8 transactions with GPT-4o-mini, Claude Haiku, and Flan-T5.\n",
        "Create a comparison table: accuracy, average response time, estimated cost per call.\n",
        "Which model offers the best value for this task?\n",
        "\n",
        "### Optional Lab 3: Streaming Responses\n",
        "Implement streaming for both OpenAI and Anthropic APIs to see responses generated\n",
        "token by token. Use `stream=True` parameter and iterate over chunks.\n",
        "\n",
        "## üî¨ Deep Dive: Attention & Transformers\n",
        "\n",
        "For hands-on exploration of how attention works, multi-head attention in PyTorch,\n",
        "and building a transformer encoder layer from scratch, see:\n",
        "\n",
        "üìì **`week_11_optional_attention_transformers.ipynb`**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-32"
      },
      "source": [
        "# Great Work Today!\n",
        "\n",
        "You've taken the first step into the GenAI world:\n",
        "\n",
        "‚úÖ Understood what LLMs are and why they matter for data science\n",
        "‚úÖ Called OpenAI and Anthropic APIs for fraud detection tasks\n",
        "‚úÖ Applied zero-shot, few-shot, and chain-of-thought prompting\n",
        "‚úÖ Extracted structured JSON data from unstructured text\n",
        "‚úÖ Used HuggingFace Flan-T5 for local text generation\n",
        "‚úÖ Discussed real-world implications: cost, privacy, reproducibility, regulation\n",
        "\n",
        "## Coming Up Next\n",
        "\n",
        "- **Week 12**: GenAI for Data Science ‚Äî synthetic data, LLM-assisted analysis, BLEU/ROUGE\n",
        "- **Week 13**: Amazon Bedrock ‚Äî managed model access and deployment\n",
        "- **Week 14**: Training AI Models ‚Äî fine-tuning with LoRA and QLoRA\n",
        "- **Weeks 15-16**: Agentic AI ‚Äî ReAct agents, LangChain, multi-agent systems\n",
        "- **Weeks 17-18**: RAG (Retrieval-Augmented Generation) ‚Äî retrieval, chunking, evaluation\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [OpenAI API Documentation](https://platform.openai.com/docs/)\n",
        "- [Anthropic API Documentation](https://docs.anthropic.com/)\n",
        "- [HuggingFace Transformers Docs](https://huggingface.co/docs/transformers/)\n",
        "- [Tokenizer Visualizer](https://tiktokenizer.vercel.app/) ‚Äî see how any text is tokenized\n",
        "- [Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) ‚Äî visual guide\n",
        "\n",
        "See you in Week 12!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62b63319e45e4ed5b6f1814f03d49c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_226f14a2774c4a3a8d9052c054e19fae",
              "IPY_MODEL_7b66a3493850474fbf1bc33f9216ae00",
              "IPY_MODEL_8f884e0b7217410aabc7030c8d313285"
            ],
            "layout": "IPY_MODEL_0b2198d04b4c441db0b125cb394f1ace"
          }
        },
        "226f14a2774c4a3a8d9052c054e19fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dfdbb62f9b942e4898bc51d6a68a577",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_64e4f282c793498b90dbd04e87bc3700",
            "value": "config.json:‚Äá100%"
          }
        },
        "7b66a3493850474fbf1bc33f9216ae00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4730c47c251f4f1f92db492a4c3abc88",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04907b2f6af8454a9e92a311442cc5bb",
            "value": 665
          }
        },
        "8f884e0b7217410aabc7030c8d313285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55a95112804749c6a91f45d66c8ea1f0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_abbf7ac1f3bf4d31b787323e6b0f1693",
            "value": "‚Äá665/665‚Äá[00:00&lt;00:00,‚Äá61.0kB/s]"
          }
        },
        "0b2198d04b4c441db0b125cb394f1ace": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dfdbb62f9b942e4898bc51d6a68a577": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64e4f282c793498b90dbd04e87bc3700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4730c47c251f4f1f92db492a4c3abc88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04907b2f6af8454a9e92a311442cc5bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55a95112804749c6a91f45d66c8ea1f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abbf7ac1f3bf4d31b787323e6b0f1693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87758d6a3e50487eb25172c990e39338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c37a0be5b0324a23ac1c078f4463ac32",
              "IPY_MODEL_df75788666884196b9fa137c07ea9c82",
              "IPY_MODEL_75dda530eb9a4eb0963589fed3329c61"
            ],
            "layout": "IPY_MODEL_5c0660ecd0e84501ab2d74311553871d"
          }
        },
        "c37a0be5b0324a23ac1c078f4463ac32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_904ac9d7310f4bb48441fd0cdda04c1f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e02edfe82718409aaf3bc7f7ee22e8f2",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "df75788666884196b9fa137c07ea9c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbe96eee9cd74a98be8d58a46a170c3c",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5c83a6a672a4368974c6dbe4e7d56d6",
            "value": 26
          }
        },
        "75dda530eb9a4eb0963589fed3329c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b328260c6914c2da5730beb6c95d7fe",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_822a1461c9c3412a8e015e59ec3161d2",
            "value": "‚Äá26.0/26.0‚Äá[00:00&lt;00:00,‚Äá1.54kB/s]"
          }
        },
        "5c0660ecd0e84501ab2d74311553871d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "904ac9d7310f4bb48441fd0cdda04c1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e02edfe82718409aaf3bc7f7ee22e8f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbe96eee9cd74a98be8d58a46a170c3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5c83a6a672a4368974c6dbe4e7d56d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b328260c6914c2da5730beb6c95d7fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "822a1461c9c3412a8e015e59ec3161d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b94a131413a04923b9207a452aa2a7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d7e05be56d64855a83dd3fa8c6f7609",
              "IPY_MODEL_c415a1afd1854961b9c33a64713de0d4",
              "IPY_MODEL_8815f5e29d2043bca3f2a16981485028"
            ],
            "layout": "IPY_MODEL_97b75c05d70344fd834b65a5947aaab2"
          }
        },
        "3d7e05be56d64855a83dd3fa8c6f7609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24ebbf46154a42d2a86aeb0c30f3c756",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_92aac135efb5408b9022fd6107b79d97",
            "value": "vocab.json:‚Äá100%"
          }
        },
        "c415a1afd1854961b9c33a64713de0d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4aaa4eab79954436adcb06d4f73ccd53",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_195dac96e464449890a2474bcc65d83d",
            "value": 1042301
          }
        },
        "8815f5e29d2043bca3f2a16981485028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a96d25be34494d00a3200687df8e1f09",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_642b4b9b76d74aea83584d34bdf5efce",
            "value": "‚Äá1.04M/1.04M‚Äá[00:00&lt;00:00,‚Äá6.82MB/s]"
          }
        },
        "97b75c05d70344fd834b65a5947aaab2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24ebbf46154a42d2a86aeb0c30f3c756": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92aac135efb5408b9022fd6107b79d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4aaa4eab79954436adcb06d4f73ccd53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "195dac96e464449890a2474bcc65d83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a96d25be34494d00a3200687df8e1f09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "642b4b9b76d74aea83584d34bdf5efce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "205fd55701b74f2ba400ccab028e5fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77af6a4d3da94b6d8e136f37eae068c7",
              "IPY_MODEL_6411ecadded44bec9f95755996601b99",
              "IPY_MODEL_a5ba10d75892494b86527c270a9a924c"
            ],
            "layout": "IPY_MODEL_3fe9b16ef954441e85b8f7a14562a4a1"
          }
        },
        "77af6a4d3da94b6d8e136f37eae068c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bede20ff5974dee8008f9a2e7e993a2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ab87d3f40a88486599d16dfaae335108",
            "value": "merges.txt:‚Äá100%"
          }
        },
        "6411ecadded44bec9f95755996601b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9691c31b0ea44d483c852cbeefea567",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a4bdcfd9a154d7bb58c5cbbca4c6f65",
            "value": 456318
          }
        },
        "a5ba10d75892494b86527c270a9a924c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f835f3d57cb43aeb507020cbe43d92b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dbbc9d13d1c5487aabc531f7bcb5ffe2",
            "value": "‚Äá456k/456k‚Äá[00:00&lt;00:00,‚Äá18.6MB/s]"
          }
        },
        "3fe9b16ef954441e85b8f7a14562a4a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bede20ff5974dee8008f9a2e7e993a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab87d3f40a88486599d16dfaae335108": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9691c31b0ea44d483c852cbeefea567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a4bdcfd9a154d7bb58c5cbbca4c6f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f835f3d57cb43aeb507020cbe43d92b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbbc9d13d1c5487aabc531f7bcb5ffe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c0d50d490a54318bb819fa9eb923ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce3586969fb44c7692aa5958ab330d40",
              "IPY_MODEL_f952fd29aaec47e69306d4df15ff00de",
              "IPY_MODEL_191a3c6b84cb4a16a844e30caa003b5f"
            ],
            "layout": "IPY_MODEL_aaf60fd6f9c044dd8a9e6dbb7b132b7c"
          }
        },
        "ce3586969fb44c7692aa5958ab330d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeb63f4da8384105b5dd2823b5774ba2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1e15ef71109b41239772cdd9027e2204",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "f952fd29aaec47e69306d4df15ff00de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dbf303ab6ea4a488ab6215a251d0b9d",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3446d40830e044a8b5f660f120dd7126",
            "value": 1355256
          }
        },
        "191a3c6b84cb4a16a844e30caa003b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6790e88c47b4c2ca2286d5813d00a43",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_82554c5c3d3b4b0cbf6fd1e0d42e385e",
            "value": "‚Äá1.36M/1.36M‚Äá[00:00&lt;00:00,‚Äá12.2MB/s]"
          }
        },
        "aaf60fd6f9c044dd8a9e6dbb7b132b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb63f4da8384105b5dd2823b5774ba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e15ef71109b41239772cdd9027e2204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3dbf303ab6ea4a488ab6215a251d0b9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3446d40830e044a8b5f660f120dd7126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6790e88c47b4c2ca2286d5813d00a43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82554c5c3d3b4b0cbf6fd1e0d42e385e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}